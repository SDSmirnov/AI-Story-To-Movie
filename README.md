# AI-Story-to-Movie (v1beta)

**AI-Story-to-Movie** is a comprehensive automated Python pipeline for converting text stories (books, scripts) into cinematic videos. The project utilizes advanced models‚ÄîGoogle Gemini, Imagen 3, and Veo‚Äîto create scripts, generate characters, render frames, and animate them.

```bash
INPUT(Story.TXT) | OUTPUT(Movie.MP4)

```

## ACHTUNG / DISCLAIMER

‚ö†Ô∏è DISCLAIMER #1: This is an experimental pipeline, not a magic "Make me an Oscar-winning movie while I drink my coffee" red button. Be prepared for manual edits, re-generations, and unexpected API costs.

‚ö†Ô∏è DISCLAIMER #2: The project works, but it requires time, patience, and money for Veo/Gemini quotas. If you want a movie "at the snap of a finger" ‚Äî this isn't for you, go to Netflix.

‚ö†Ô∏è DISCLAIMER #3: If the project doesn't work for you, then "Keep sawing, Shura, keep sawing" (Russian idiom meaning: keep grinding/debugging until it works).

‚ö†Ô∏è DISCLAIMER #4: Project Status: Proof of Concept. Not maintained. API compatibility not guaranteed.

## üé¨ Result Demo

Watch an example of a film fully generated by this pipeline based on Ted Chiang's short story "Exhalation".

[![AI Film: Exhalation)](https://img.youtube.com/vi/l7-UIGjO43Q/0.jpg)](https://www.youtube.com/watch?v=l7-UIGjO43Q)

---

## üöÄ Key Features

* **Style Master:** Automatically determines the genre, atmosphere, and visual style based on the text of the story.
* **Auto-Casting:** Identifies key characters, creates detailed descriptions for them, and generates reference images to maintain facial consistency across different scenes.
* **Cinematic Storyboarding:** Breaks the text into scenes and panels, generating `Start` (action beginning) and `End` (result) frame pairs for smooth animation.
* **Video Generation (Google Veo):** Turns static frame pairs into 4-8 second video clips with high temporal consistency.
* **Smart Dubbing:** Creates an audio track with role distribution (narrator, characters) and SFX (sound effects) generation via ElevenLabs.
* **Flexible Styling:** Supports presets: Realism, Anime, Comic Book, Graphic Novel, etc.

---

## üèó Project Architecture

The process is divided into 5 sequential stages:

1. üé© **Style Master (`00_style_master.py`)**: Text analysis and system prompt generation for the selected style.
2. üé¨ **Cinematic Preroll (`01_cinematic_preroll.py`)**: The "Director" stage ‚Äî casting, generating Keyframes and scene metadata.
3. üìπ **Image Animator (`02_image_animator.py`)**: The "Camera Operator" stage ‚Äî frame animation using the Veo model.
4. üéôÔ∏è **Sound Producer (`03_sound_producer.py`)**: The "Sound Engineer" stage ‚Äî speech generation (TTS), sound effects (SFX), and EDL (Edit Decision List) creation.
5. üéõÔ∏è **Audio Assembler (`04_audio_assembler.py`)**: Mixing the final audio file.

---

## üõ† Requirements

* **Python 3.10+**
* **FFmpeg** (installed and available in PATH).
* API Keys:
* `IMG_AI_API_KEY` / `GOOGLE_API_KEY` (Google Gemini/Veo).
* `ELEVEN_API_KEY` (Optional, for SFX).



### Installing Dependencies

```bash
pip install google-generativeai pillow pydub moviepy elevenlabs

```

---

## üìñ Usage

### 1. Generating Stylistic Prompts

Analyzes the book text and creates a configuration (`style.md`, `casting.md`, etc.) in the `custom_prompts` folder.

```bash
python src/00_style_master.py path/to/story.txt --style realistic_movie

```

*Available styles:* `realistic_movie`, `anime`, `comic_book`, `graphic_novel`, `watchmen_style`.

### 2. Creating Scenes and Frames (Preroll)

Generates character references and images for all scenes.
Use the `--custom-prompts` flag to apply settings from step 1.

```bash
python src/01_cinematic_preroll.py path/to/story.txt --custom-prompts

```

*Result:* Images in `cinematic_render/panels` and metadata in `animation_metadata.json`.

### 3. Image Animation

Turns image pairs (Start/End) into video clips using Google Veo.

```bash
python src/02_image_animator.py

```

*Result:* MP4 files in `cinematic_render/clips`.

### 4. Sound Generation

Creates character lines and sound effects based on the script.

```bash
python src/03_sound_producer.py path/to/story.txt

```

*Result:* Audio files and EDL JSON in `cinematic_render/audio_master`.

### 5. Audio Assembly

Mixes all sounds into a single file based on the EDL (Edit Decision List).

```bash
python src/04_audio_assembler.py cinematic_render/audio_master/scene_001_audio_EDL.json final_audio.mp3

```

*Result:* Finished audio file `final_audio.mp3`.

---

## ‚öôÔ∏è Configuration

You can manually adjust generation parameters in the `src/custom_prompts/config.json` file:

```json
{
  "format": {
    "type": "single_grid_animation",
    "panels_per_scene": 9
  },
  "image_generation": {
    "resolution": "2K",
    "aspect_ratio": "16:9"
  },
  "animation": {
    "enabled": true,
    "keyframe_type": "start_end"
  }
}

```

---

## üìÇ Project Structure

```text
.
‚îú‚îÄ‚îÄ demo/                   # Example data (Isaac Asimov "Reason")
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ 00_style_master.py      # Style Analyzer
‚îÇ   ‚îú‚îÄ‚îÄ 01_cinematic_preroll.py # Frame Generator
‚îÇ   ‚îú‚îÄ‚îÄ 02_image_animator.py    # Animator (Veo)
‚îÇ   ‚îú‚îÄ‚îÄ 03_sound_producer.py    # Sound Generator
‚îÇ   ‚îú‚îÄ‚îÄ 04_audio_assembler.py   # Sound Assembler
‚îÇ   ‚îú‚îÄ‚îÄ custom_prompts/         # Generated prompts
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                # Prompt templates
‚îÇ   ‚îî‚îÄ‚îÄ tools/                  # Utilities (ffmpeg wrapper, smart-dubbing)
‚îî‚îÄ‚îÄ cinematic_render/       # Output folder (created automatically)
    ‚îú‚îÄ‚îÄ panels/             # Generated images
    ‚îú‚îÄ‚îÄ clips/              # Finished video fragments
    ‚îú‚îÄ‚îÄ audio_master/       # Audio and EDL files
    ‚îî‚îÄ‚îÄ animation_metadata.json # Main script file

```

---

## ‚ö†Ô∏è Important Notes

1. **API Quotas:** Video generation via Veo and image generation via Imagen consume API quotas. The script handles Error 429 (Resource Exhausted) and suggests continuing later.
2. **Quality Protocol:** The system includes a "Self-Criticism Protocol" (aka the "Shit, redo protocol"), forcing the model to double-check its responses to improve script quality.
3. **Ref Dir:** The `ref_thriller` folder (or similar) is used to store character references. If a character is already there, a new one will not be generated.

---

## üí° Know-how's & How-to's

The pipeline is built on several non-trivial engineering and prompt-engineering solutions. Understanding these mechanisms will allow you to get "cinema" quality results, not just a slide show.

### 1. The "It's Crap, Redo It" Protocol (Self-Correction Protocol)

In `01_cinematic_preroll.py`, an aggressive system prompt is used for the LLM. We don't just ask the model to break the text into scenes; we force it to go through 4 iterations of self-criticism before outputting JSON:

> *"1. Understand why your answer is crap... 2. Redo it... 3. Check it again..."*

**Tip:** If the scenes turn out too simple, don't change the code ‚Äî just increase the "degree" of criticism in the `SYSTEM_E` variable inside the script.

### 2. Character Consistency (Auto-Casting Hack)

The biggest problem with AI video is that characters change faces from scene to scene.

* **How it works:** The script first generates "passport photos" of the characters and saves them in the `ref_thriller` folder. When generating scenes, these photos are fed to the model (Imagen/Gemini) as references.
* **The Trick:** You can **manually replace the files** in the `ref_thriller` folder with your own (e.g., photos of real actors or better Midjourney generations). The main thing is to keep the filenames. The pipeline will automatically pick up your references for all future scenes.

### 3. The Dual-Grid Method for Veo

The Google Veo model (like Runway/Luma) works better when it understands the beginning and end of a movement.

* **Logic:** We generate **one** 4K image, in which two frames are stacked vertically: Top ‚Äî Start Frame, Bottom ‚Äî End Frame.
* **Why:** This guarantees that the lighting, scenery, and character's clothing are identical at the beginning and end of the clip. The model only has to interpolate the movement, minimizing background hallucinations.

### 4. "Acting" in TTS (Hidden Instructions)

Standard TTS reads text monotonously. We use Gemini TTS with a "director's prompt".

* **Mechanics:** We feed the model text like: `[Whispering, with horror]: It held the beam!`. The system prompt forbids the model from reading the text in brackets aloud but forces it to apply that emotion to the audio generation.
* **Tip:** In the `smart-dubbing` file, you can manually tweak the JSON with the dubbing plan (`dubbing_plan.json`) by adding your own emotional remarks before generating audio.

### 5. Saving Tokens and Money

Video generation is an expensive pleasure.

* **Smart References:** The `02_image_animator.py` script analyzes the scene before sending it to Veo. If a character is standing with their back turned or is far away, the script **does not send** the face reference to the API, saving expensive context tokens.
* **Caching:** All scripts check for the existence of output files. If `clip_001.mp4` already exists and is not empty, generation is skipped. To redo a scene, simply delete the specific file.

### 6. Human-in-the-loop

The pipeline is designed as a modular construction set. You can stop the process at any stage:

1. **After Preroll:** Open `cinematic_render/panels`, delete failed frames, or redraw them in Photoshop/Inpainting.
2. **Before Animation:** Tweak `animation_metadata.json` by changing movement prompts (`motion_prompt`) if Veo doesn't understand the task.
3. **Before Mixing:** Replace individual audio files in `audio_master` with your own voice recordings, keeping the filenames. The assembler will stitch everything together perfectly.

### 7. The "JUST BEFORE ACTION" Principle

When generating prompts for panels (images), it is critically important to describe not the action itself, but the **moment before it**.

* *Bad:* "The robot punches the table." (The static image will be blurry or illogical).
* *Good:* "The robot raised its fist above the table in a fit of rage, a split second before the strike."
This gives the video generation model (Veo) "potential energy," which it naturally converts into motion.

---

## üìä Case Study: "Reason" (Isaac Asimov)

*Real metrics from creating a 14-minute film based on v1beta.*

**Timing and Costs:**

* **Final Runtime:** 14 minutes.
* **Production Time:** 3 evenings (one person).
* **Work Volume:** 19 scenes (3√ó3 format), 171 keyframes.

**Generation Statistics:**

* **Animation (Google Veo API):** ~75 shots.
* *Veo 3.1 Fast:* 70 shots (bulk).
* *Veo 3.1 (Full):* 5 shots (complex scenes with characters missing from the frame).
* *Failures (Moderation/Fail):* ~7% (5 out of 70).


* **Manual Refinement (External Tools):** ~96 shots (Grok Imagine/Runway to avoid paying for Veo API).
* **Refinement:**
* *Scene Regeneration:* 5 out of 19 (casting bugs, random people in locations).
* *Frame Edits:* ~10% (16 out of 171) ‚Äî refining faces and details.



**Sound Pipeline:**

Whisper (original timing recognition) ‚Üí Gemini Translation (translation) ‚Üí Gemini TTS (dubbing) ‚Üí Shotcut (mixing). The result requires minimal edits in post.

## üé¨ Full Production Stack

*Example from the creation of the film "Reason" (I. Asimov)*

* üêç **Core:** Python (`AI-Story-to-Movie` pipeline).
* üß† **Script and Casting:** Gemini 2.5 Pro.
* üé® **Storyboard:** Imagen 3 / Nano Banana Pro.
* üé• **Animation (VFX):** Google Veo 3.1 (API) + Grok Imagine (Refinements).
* üó£Ô∏è **Voice (Voices):** Gemini 2.5 Flash TTS.
* üîä **Sound Effects (SFX):** ElevenLabs.
* üéµ **Music (OST):** Suno AI.
* ‚úÇÔ∏è **Post-production:** Shotcut + FFmpeg.

---

## TODO

AI Critic Agents are still needed:

1. **Scene Breakdown Critic:** "Does this set of scenes fully convey the story? Is too little/much time spent here?" ‚Äî split/consolidate.
2. **Storyboard Critic:** "Are there any critical hallucinations here?" ‚Äî redraw panel.
3. **Animation Critic:** "Did the model glitch here? Maybe this needs to be sent without censorship in the sendframe?" ‚Äî regenerate animation.
4. **Sound Mixing Critic:** "Is the sound effect on time here? Does the mouth speak earlier/later? Too much/little text?" ‚Äî rewrite dialogues for dubbing.
For POC/MVP, the critic role is currently played by a human, but the pipeline architecture allows for the implementation of critic roles. Yes, it will be slower and more expensive, but the result will be better. But that's not for sure üòâ

---

**License:** WTFPL
**(c) 2026, E.Z. AI-Story-to-Movie Project**

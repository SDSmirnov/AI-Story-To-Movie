#!/bin/env python3
import os
import sys
import json
import wave
import math
import shutil
import time
import numpy as np
from pathlib import Path

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è AI
from google import genai
from google.genai import types
from faster_whisper import WhisperModel

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –º–µ–¥–∏–∞
from moviepy.editor import VideoFileClip
from pydub import AudioSegment

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
INPUT_VIDEO = sys.argv[1]
OUTPUT_FILENAME = sys.argv[2]
CONTEXT = open(sys.argv[3], 'r').read()

INTERMEDIATE_JSON = "dubbing_plan.json"
TRANSCRIPTION_CACHE = "transcription_cache.json"
TEMP_WAV = "temp_source.wav"
TEMP_SEGMENTS_DIR = Path("temp_segments")

# –ú–æ–¥–µ–ª–∏
TTS_MODEL = "gemini-2.5-flash-preview-tts"
TRANSLATION_MODEL = "gemini-2.5-pro"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ—á–∏
TARGET_WPM = 130  # —Å–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
SPEED_TOLERANCE = 1.35  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞—â–∏—Ç—ã –æ—Ç –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç–∞
MIN_GAP_MS = 50  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∑–∞–∑–æ—Ä –º–µ–∂–¥—É —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
OVERLAP_DETECTION_THRESHOLD = 100  # –µ—Å–ª–∏ –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç –±–æ–ª—å—à–µ 100–º—Å, —Å—á–∏—Ç–∞–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–º

# –ú–∞–ø–ø–∏–Ω–≥ –≥–æ–ª–æ—Å–æ–≤
VOICE_MAP = {
    "narrator": "Rasalgethi",
    "narrator_drama": "Fenrir",
    "narrator_soft": "Vindemiatrix",
    "male_hero": "Orus",
    "male_deep": "Puck",
    "male_calm": "Umbriel",
    "male": "Puck",
    "female_hero": "Zephyr",
    "female_strict": "Kore",
    "female_soft": "Achernar",
    "female": "Zephyr",
}

if not GOOGLE_API_KEY:
    raise ValueError("‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω GOOGLE_API_KEY")

client = genai.Client(api_key=GOOGLE_API_KEY)

def estimate_speech_duration(text, wpm=TARGET_WPM):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –∏—Å—Ö–æ–¥—è –∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤"""
    words = len(text.split())
    duration_seconds = (words / wpm) * 60
    return duration_seconds

def calculate_max_words(duration_seconds, wpm=TARGET_WPM):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    return int((duration_seconds * wpm) / 60)

def get_file_hash(filepath):
    """–ü—Ä–æ—Å—Ç–æ–π —Ö–µ—à –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ñ–∞–π–ª–∞"""
    import hashlib
    stat = os.stat(filepath)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–º–µ—Ä –∏ –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞–∫ "—Ö–µ—à"
    hash_string = f"{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(hash_string.encode()).hexdigest()

# ==========================================
# 1. –¢–†–ê–ù–°–ö–†–ò–ë–ê–¶–ò–Ø (WHISPER) –° –ö–ï–®–ò–†–û–í–ê–ù–ò–ï–ú
# ==========================================
def transcribe_video(video_path):
    print(f"üéß 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è (Whisper)...")

    # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à –≤–∏–¥–µ–æ –¥–ª—è –∫–µ—à–∞
    video_hash = get_file_hash(video_path)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
    cache_valid = False
    if os.path.exists(TRANSCRIPTION_CACHE):
        try:
            with open(TRANSCRIPTION_CACHE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                if cache_data.get('video_hash') == video_hash:
                    print(f"   üíæ –ù–∞–π–¥–µ–Ω –∫–µ—à —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ!")
                    segments = cache_data.get('segments', [])
                    total_duration = cache_data.get('total_duration', 0)
                    cache_valid = True
                    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–µ—à–∞")
                else:
                    print(f"   ‚ö†Ô∏è –ö–µ—à —É—Å—Ç–∞—Ä–µ–ª (–≤–∏–¥–µ–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å), –≤—ã–ø–æ–ª–Ω—è–µ–º –Ω–æ–≤—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é...")
        except (json.JSONDecodeError, KeyError) as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")

    if not cache_valid:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π WAV
        if os.path.exists(TEMP_WAV):
            print(f"   üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π {TEMP_WAV}")
            clip = VideoFileClip(video_path)
            total_duration = clip.duration
            clip.close()
        else:
            print(f"   üé¨ –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ...")
            clip = VideoFileClip(video_path)
            clip.audio.write_audiofile(TEMP_WAV, logger=None)
            total_duration = clip.duration
            clip.close()

        # Whisper
        print(f"   ü§ñ –ó–∞–ø—É—Å–∫ Whisper –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏...")
        model = WhisperModel("medium", device="cpu", compute_type="int8")
        segments_raw, _ = model.transcribe(
            TEMP_WAV,
            beam_size=5,
            language="en",
            word_timestamps=True,  # –í–∫–ª—é—á–∞–µ–º —Ç–æ—á–Ω—ã–µ —Ç–∞–π–º–∫–æ–¥—ã —Å–ª–æ–≤
            vad_filter=True,       # –í–∫–ª—é—á–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π VAD —Ñ–∏–ª—å—Ç—Ä Whisper
            vad_parameters=dict(min_silence_duration_ms=500))

        segments = []
        for s in segments_raw:
            segments.append({
                "start": s.start,
                "end": s.end,
                "original_text": s.text.strip()
            })
            print(f"   {segments[-1]}")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        cache_data = {
            'video_hash': video_hash,
            'video_path': video_path,
            'total_duration': total_duration,
            'segments': segments,
            'timestamp': time.time()
        }
        with open(TRANSCRIPTION_CACHE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        print(f"   üíæ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –∫–µ—à: {TRANSCRIPTION_CACHE}")

    return segments, total_duration

# ==========================================
# 2. –ü–ï–†–ï–í–û–î –ò –ê–ù–ê–õ–ò–ó –≠–ú–û–¶–ò–ô (GEMINI)
# ==========================================
def analyze_and_translate(segments):
    print(f"üß† 2. –ü–µ—Ä–µ–≤–æ–¥ –∏ –∞–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π (Gemini)...")

    prompt_text = f"""
You are a professional dubbing director for Russian dubbing.

For EACH sentence, you must:
1. Translate it to Russian (natural conversational style).
2. ADAPT the translation length to fit the time slot based on word count.
3. Detect Emotion/Tone (e.g., 'excited', 'sad', 'neutral', 'whispering', 'shouting').
4. Suggest Voice Type (choose from: 'male_hero', 'male_deep', 'male_calm', 'female_hero', 'female_soft', 'female_strict', 'narrator').
5. Assign 'speaker_id' for voice consistency tracking (optional field).

CRITICAL TIMING RULES:
- Russian speech rate: {TARGET_WPM} words per minute
- Each segment has 'duration' in seconds
- Calculate max_words = (duration * {TARGET_WPM}) / 60
- Your translation MUST NOT exceed max_words
- If original is too long, PARAPHRASE to fit (keep meaning, reduce words)
- Prioritize natural Russian phrasing over literal translation

VOICE CONSISTENCY BY PERSONA:
- Analyze the context to identify distinct speakers/characters
- If you detect multiple personas (e.g., interviewer/interviewee, host/guest, different characters), assign each a unique 'speaker_id' (e.g., "person_1", "person_2", "character_alice")
- ALWAYS use the SAME voice_type for the SAME speaker_id throughout the entire video
- For single-speaker content (monologue, lecture, documentary narration), use speaker_id: "narrator" or null
- Consider gender, age, and character traits when assigning voice types to personas
- Examples:
  * Interview: speaker_id "interviewer" ‚Üí male_calm, speaker_id "guest" ‚Üí female_hero
  * Dialogue: speaker_id "hero" ‚Üí male_hero, speaker_id "villain" ‚Üí male_deep
  * Documentary: speaker_id "narrator" ‚Üí narrator

Input: JSON list with 'id', 'text', 'duration', 'max_words'
Output: RAW JSON list with 'id', 'ru_text', 'word_count', 'tone', 'voice_type', 'speaker_id' (optional)

DO NOT use Markdown code blocks. Just raw JSON.

<CONTEXT_STORY>{CONTEXT}</CONTEXT_STORY>
"""

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    input_data = []
    for i, s in enumerate(segments):
        duration = s['end'] - s['start']
        max_words = calculate_max_words(duration)
        input_data.append({
            "id": i,
            "text": s["original_text"],
            "duration": round(duration, 2),
            "max_words": max_words
        })

    response = client.models.generate_content(
        model=TRANSLATION_MODEL,
        contents=f"{prompt_text}\n\nDATA: {json.dumps(input_data, ensure_ascii=False)}",
        config=types.GenerateContentConfig(response_mime_type="application/json")
    )

    try:
        translated_data = json.loads(response.text)
        trans_map = {item['id']: item for item in translated_data}

        combined = []
        for i, seg in enumerate(segments):
            t_item = trans_map.get(i)
            if t_item:
                ru_text = t_item.get("ru_text", "")
                seg["ru_text"] = ru_text
                seg["word_count"] = len(ru_text.split())
                seg["tone"] = t_item.get("tone", "neutral")
                seg["voice_type"] = t_item.get("voice_type", "narrator")
                seg["speaker_id"] = t_item.get("speaker_id", None)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–µ–≤–æ–¥
                duration = seg['end'] - seg['start']
                max_words = calculate_max_words(duration)
                if seg["word_count"] > max_words:
                    print(f"   ‚ö†Ô∏è –°–µ–≥–º–µ–Ω—Ç {i}: {seg['word_count']} —Å–ª–æ–≤ > {max_words} (–º–æ–∂–µ—Ç –Ω–µ –≤–ª–µ–∑—Ç—å)")
            else:
                seg["ru_text"] = seg["original_text"]
                seg["word_count"] = len(seg["original_text"].split())
                seg["tone"] = "neutral"
                seg["voice_type"] = "narrator"
                seg["speaker_id"] = None
            combined.append(seg)

        return combined
    except json.JSONDecodeError:
        print("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Gemini.")
        print(response.text)
        return segments

# ==========================================
# 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ß–ò (GEMINI TTS)
# ==========================================
def generate_audio_segment(text, voice_key, tone, output_path):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"""

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    if output_path.exists():
        print(f"   üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–∞–π–ª: {output_path.name}")
        return True

    gemini_voice = VOICE_MAP.get(voice_key, "Rasalgethi")

    speech_config = types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
                voice_name=gemini_voice
            )
        )
    )

    prompt = f"""Read the following text in Russian.
EMOTION/TONE: {tone}
TEXT TO READ: {text}
INSTRUCTION: Apply the emotion naturally, but do not read these instructions aloud."""

    try:
        time.sleep(5)  # Rate limiting
        response = client.models.generate_content(
            model=TTS_MODEL,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=["AUDIO"],
                speech_config=speech_config
            )
        )

        for part in response.candidates[0].content.parts:
            if part.inline_data:
                with wave.open(str(output_path), 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(24000)
                    wav_file.writeframes(part.inline_data.data)
                return True
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ TTS –¥–ª—è '{text[:30]}...': {e}")
        return False

# ==========================================
# 4. –°–ë–û–†–ö–ê (PYDUB) –° –ó–ê–©–ò–¢–û–ô –û–¢ –ü–ï–†–ï–•–õ–Å–°–¢–ê
# ==========================================
def detect_and_resolve_overlaps(segments):
    """
    –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç—ã.
    –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–∞—á–∞–ª–∞ (start) —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞.
    –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Ö–ª–µ—Å—Ç, —É–∫–æ—Ä–∞—á–∏–≤–∞–µ–º –∫–æ–Ω–µ—Ü (end) –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ.
    """
    print(f"üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç–æ–≤ (STRATEGY: Prioritize Next Start)...")

    resolved_segments = []
    if not segments:
        return []

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç –∫–∞–∫ –µ—Å—Ç—å
    resolved_segments.append(segments[0].copy())

    for i in range(1, len(segments)):
        current_seg = segments[i].copy()
        prev_seg = resolved_segments[-1]

        # –ì—Ä–∞–Ω–∏—Ü—ã —Å —É—á–µ—Ç–æ–º —É–∂–µ –≤–Ω–µ—Å–µ–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
        prev_end = prev_seg.get('adjusted_end', prev_seg['end'])
        current_start = current_seg['start']

        # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –†–ê–ù–¨–®–ï, —á–µ–º –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–µ–¥—ã–¥—É—â–∏–π (—Å —É—á–µ—Ç–æ–º –∑–∞–∑–æ—Ä–∞)
        if current_start < prev_end + (MIN_GAP_MS / 1000.0):
            overlap_amount = (prev_end + (MIN_GAP_MS / 1000.0)) - current_start

            # –†–ï–®–ï–ù–ò–ï: –ü–æ–¥—Ä–µ–∑–∞–µ–º –ü–†–ï–î–´–î–£–©–ò–ô —Å–µ–≥–º–µ–Ω—Ç, —á—Ç–æ–±—ã –æ—Å–≤–æ–±–æ–¥–∏—Ç—å –º–µ—Å—Ç–æ –¥–ª—è –Ω–æ–≤–æ–≥–æ
            # –ù–æ –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ–º –ø–æ–¥—Ä–µ–∑–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç –¥–æ –Ω—É–ª—è (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ö–æ—Ç—è –±—ã 0.5 —Å–µ–∫)
            min_duration = 0.5
            new_prev_end = current_start - (MIN_GAP_MS / 1000.0)

            prev_start = prev_seg.get('adjusted_start', prev_seg['start'])

            if (new_prev_end - prev_start) > min_duration:
                print(f"   ‚úÇÔ∏è –û–±—Ä–µ–∑–∞–µ–º –∫–æ–Ω–µ—Ü —Å–µ–≥–º–µ–Ω—Ç–∞ {i-1}, —á—Ç–æ–±—ã —Å–ø–∞—Å—Ç–∏ —Å–∏–Ω—Ö—Ä–æ–Ω —Å–µ–≥–º–µ–Ω—Ç–∞ {i}")
                prev_seg['adjusted_end'] = round(new_prev_end, 3)
                # –¢–µ–∫—É—â–∏–π –æ—Å—Ç–∞–≤–ª—è–µ–º –≥–¥–µ –±—ã–ª (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω –≥—É–±/–¥–µ–π—Å—Ç–≤–∏—è)
                current_seg['adjusted_start'] = current_start
            else:
                # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, —á—Ç–æ–±—ã –µ–≥–æ —Ä–µ–∑–∞—Ç—å,
                # —Ç–æ–≥–¥–∞, —É–≤—ã, —Å–¥–≤–∏–≥–∞–µ–º —Ç–µ–∫—É—â–∏–π (–º–µ–Ω—å—à–µ–µ –∏–∑ –∑–æ–ª)
                print(f"   üê¢ –°–¥–≤–∏–≥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç {i} (–ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π)")
                current_seg['adjusted_start'] = round(prev_end + (MIN_GAP_MS / 1000.0), 3)
        else:
            current_seg['adjusted_start'] = current_start

        current_seg['adjusted_end'] = current_seg['end'] # –ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–π –∫–æ–Ω–µ—Ü
        resolved_segments.append(current_seg)

    return resolved_segments

def assemble_audio(segments, total_duration):
    print(f"üéπ 4. –°–±–æ—Ä–∫–∞ –∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ...")

    # –°–Ω–∞—á–∞–ª–∞ —É—Å—Ç—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç—ã
    segments = detect_and_resolve_overlaps(segments)

    final_track = AudioSegment.silent(duration=int(total_duration * 1000))
    TEMP_SEGMENTS_DIR.mkdir(exist_ok=True)

    placement_log = []

    for i, seg in enumerate(segments):
        if not seg["ru_text"] or len(seg["ru_text"]) < 2:
            continue

        temp_file = TEMP_SEGMENTS_DIR / f"seg_{i}_{seg['voice_type']}.wav"

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º (–∏–ª–∏ –±–µ—Ä–µ–º –∏–∑ –∫—ç—à–∞)
        speaker_info = f"[{seg.get('speaker_id', 'unknown')}]" if seg.get('speaker_id') else ""
        print(f"   üéôÔ∏è [{i}]{speaker_info}[{seg['voice_type']}][{seg['tone']}] {seg['ru_text'][:50]}...")
        success = generate_audio_segment(seg['ru_text'], seg['voice_type'], seg['tone'], temp_file)

        if success:
            audio_segment = AudioSegment.from_wav(str(temp_file))
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
            segment_start = seg.get('adjusted_start', seg['start'])

            # –í–ê–ñ–ù–û: –ï—Å–ª–∏ –º—ã –ø–æ–¥—Ä–µ–∑–∞–ª–∏ –∫–æ–Ω–µ—Ü —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ detect_and_resolve_overlaps,
            # –º—ã –¥–æ–ª–∂–Ω—ã —É—á–µ—Å—Ç—å —ç—Ç–æ –∑–¥–µ—Å—å
            target_end = seg.get('adjusted_end', seg['end'])

            # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ –≤—Ä–µ–º—è
            time_slot_duration_ms = (target_end - segment_start) * 1000
            actual_duration_ms = len(audio_segment)

            start_pos = int(segment_start * 1000)

            # –õ–æ–≥–∏–∫–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—è (Speedup)
            # –ï—Å–ª–∏ –∞—É–¥–∏–æ –¥–ª–∏–Ω–Ω–µ–µ —Å–ª–æ—Ç–∞ (–ø–æ—Ç–æ–º—É —á—Ç–æ –º—ã –µ–≥–æ –ø–æ–¥—Ä–µ–∑–∞–ª–∏ –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –¥–ª–∏–Ω–Ω—ã–π)
            if actual_duration_ms > time_slot_duration_ms:
                speed_factor = actual_duration_ms / time_slot_duration_ms

                # –†–∞–∑—Ä–µ—à–∞–µ–º —á—É—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ, –µ—Å–ª–∏ —Å–ª–æ—Ç –±—ã–ª –ø–æ–¥—Ä–µ–∑–∞–Ω
                if speed_factor > SPEED_TOLERANCE:
                     # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ, —É—Å–∫–æ—Ä—è–µ–º –¥–æ –ø—Ä–µ–¥–µ–ª–∞,
                     # –Ω–æ –∞—É–¥–∏–æ "–Ω–∞–µ–¥–µ—Ç" –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π —Å–µ–≥–º–µ–Ω—Ç (–ª—É—á—à–µ —Ç–∞–∫, —á–µ–º —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω)
                    speed_factor = SPEED_TOLERANCE

                if speed_factor > 1.05:
                    audio_segment = audio_segment.speedup(playback_speed=speed_factor)
                    actual_duration_ms = len(audio_segment)

            # –ù–∞–ª–æ–∂–µ–Ω–∏–µ
            final_track = final_track.overlay(audio_segment, position=start_pos)

            placement_log.append({
                'segment_id': i,
                'start_pos': start_pos,
                'end_pos': start_pos + actual_duration_ms,
                'duration': actual_duration_ms
            })

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–æ–≥ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    with open("placement_log.json", "w", encoding="utf-8") as f:
        json.dump(placement_log, f, ensure_ascii=False, indent=2)
    print(f"   üìã –õ–æ–≥ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ placement_log.json")

    return final_track

# ==========================================
# MAIN
# ==========================================
def main():
    print("üé¨ Smart Dubbing Script - Enhanced Version v2.0")
    print(f"   Target WPM: {TARGET_WPM}")
    print(f"   Max speed: {SPEED_TOLERANCE}x")
    print(f"   Min gap between segments: {MIN_GAP_MS}ms")
    print()

    # –®–∞–≥ 1: Whisper —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü
    segments, duration = transcribe_video(INPUT_VIDEO)

    # –®–∞–≥ 2: Gemini —Å —É—á–µ—Ç–æ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if os.path.exists(INTERMEDIATE_JSON):
        print(f"üìÇ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª {INTERMEDIATE_JSON}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ.")
        with open(INTERMEDIATE_JSON, "r", encoding="utf-8") as f:
            rich_segments = json.load(f)
    else:
        rich_segments = analyze_and_translate(segments)
        with open(INTERMEDIATE_JSON, "w", encoding="utf-8") as f:
            json.dump(rich_segments, f, ensure_ascii=False, indent=2)
        print(f"üíæ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {INTERMEDIATE_JSON}")

    # –®–∞–≥ 3-4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∫—ç—à–µ–º –∏ —Å–±–æ—Ä–∫–∞ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ—Ö–ª—ë—Å—Ç–∞
    final_audio = assemble_audio(rich_segments, duration)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_audio.export(OUTPUT_FILENAME, format="mp3")
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª: {OUTPUT_FILENAME}")
    print(f"   –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è")
    print(f"   üìÅ –ö–µ—à —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏: {TRANSCRIPTION_CACHE}")
    print(f"   üìÅ –ü–ª–∞–Ω –¥—É–±–ª—è–∂–∞: {INTERMEDIATE_JSON}")
    print(f"   üìÅ –õ–æ–≥ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è: placement_log.json")

if __name__ == "__main__":
    main()

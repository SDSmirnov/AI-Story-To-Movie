import os
import json
import time
import glob
import subprocess
import argparse
import sys
import wave
from pathlib import Path
from typing import List, Dict, Any, Optional

# –°—Ç–æ—Ä–æ–Ω–Ω–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from google import genai
from google.genai import types

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ElevenLabs —Ç–æ–ª—å–∫–æ –¥–ª—è SFX (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from elevenlabs.client import ElevenLabs
    from elevenlabs import save
    HAS_ELEVEN = True
except ImportError:
    HAS_ELEVEN = False
    print("‚ö†Ô∏è ElevenLabs SDK –Ω–µ –Ω–∞–π–¥–µ–Ω. SFX –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç.")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
# –ú–æ–¥–µ–ª—å –¥–ª—è –ª–æ–≥–∏–∫–∏/—Ä–µ–∂–∏—Å—Å—É—Ä—ã (–Ω—É–∂–µ–Ω Pro –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∞–∂–∞)
GEMINI_MODEL_LOGIC = "gemini-2.5-pro"
# –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏ (TTS)
GEMINI_MODEL_TTS = "gemini-2.5-flash-preview-tts"

ELEVEN_API_KEY = os.getenv("ELEVEN_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# –ü—É—Ç–∏
CLIPS_DIR = Path("cinematic_render/clips")
OUTPUT_AUDIO_DIR = Path("cinematic_render/audio_master")
METADATA_FILE = Path("cinematic_render/animation_metadata.json")
ORIGINAL_TEXT_FILE = Path(sys.argv[1] if len(sys.argv) > 1 else "script.txt")

OUTPUT_AUDIO_DIR.mkdir(parents=True, exist_ok=True)

# –ö–∞—Ä—Ç–∞ –≥–æ–ª–æ—Å–æ–≤ Gemini (—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ä–æ–ª–µ–π —Å –º–æ–¥–µ–ª—è–º–∏ –≥–æ–ª–æ—Å–æ–≤)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ–ª–æ—Å–∞ –∏–∑ improved_audiobook.py
VOICE_MAP = {
    "Narrator": "Rasalgethi",       # –°–ø–æ–∫–æ–π–Ω—ã–π, –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π
    "Woman":    "Zephyr",           # –ñ–µ–Ω—Å–∫–∏–π –º—è–≥–∫–∏–π
    "Robot":    "Iapetus",          # –ú–µ—Ö–∞–Ω–∏—á–µ—Å–∫–∏–π
    "SFX":      "sfx_generator_v1"  # –ú–∞—Ä–∫–µ—Ä –¥–ª—è ElevenLabs SFX
}

if not GOOGLE_API_KEY:
    print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ –∑–∞–¥–∞–Ω GOOGLE_API_KEY")
    exit(1)

client_gemini = genai.Client(api_key=GOOGLE_API_KEY)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ElevenLabs —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á (–¥–ª—è SFX)
client_eleven = None
if HAS_ELEVEN and ELEVEN_API_KEY:
    client_eleven = ElevenLabs(api_key=ELEVEN_API_KEY)

# ==========================================
# RESPONSE SCHEMA –î–õ–Ø GEMINI
# ==========================================
SAFETY = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
]
AUDIO_PLAN_SCHEMA = types.GenerateContentConfig(
    safety_settings=SAFETY,
    response_mime_type="application/json",
    response_schema={
        "type": "object",
        "properties": {
            "audio_events": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {
                            "type": "string",
                            "description": "Unique identifier for audio event (e.g. audio_001_event_01)"
                        },
                        "type": {
                            "type": "string",
                            "enum": ["speech", "sfx"],
                            "description": "Type of audio: speech or sound effect"
                        },
                        "voice": {
                            "type": "string",
                            "description": "Character key from VOICE_MAP (Narrator, Hermes, etc.)"
                        },
                        "text": {
                            "type": "string",
                            "description": "The exact dialogue text in Russian."
                        },
                        "prompt": {
                            "type": "string",
                            "description": "For SFX only: Description of the sound."
                        },
                        "sound_start_time": {
                            "type": "number",
                            "description": "Start time in seconds (absolute time from 0.0)"
                        },
                        "sound_end_time": {
                            "type": "number",
                            "description": "End time in seconds"
                        },
                        "notes": {
                            "type": "string",
                            "description": "Critical for TTS: Emotion, tone, and delivery instructions (e.g. 'Whispering with fear', 'Sarcastic', 'Loud command')."
                        }
                    },
                    "required": ["id", "type", "sound_start_time", "sound_end_time"]
                }
            }
        },
        "required": ["audio_events"]
    }
)

# ==========================================
# 1. –°–ë–û–†–ö–ê –í–ò–ó–£–ê–õ–¨–ù–û–ì–û –¢–ê–ô–ú–õ–ê–ô–ù–ê
# ==========================================

def get_video_duration(filepath: Path) -> float:
    try:
        result = subprocess.run(
            ["ffprobe", "-v", "error", "-show_entries", "format=duration",
             "-of", "default=noprint_wrappers=1:nokey=1", str(filepath)],
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT
        )
        return float(result.stdout)
    except:
        return 4.0 # Fallback

def build_scene_structure(metadata: dict) -> List[Dict]:
    print("‚è≥ –ê–Ω–∞–ª–∏–∑ —Ö—Ä–æ–Ω–æ–º–µ—Ç—Ä–∞–∂–∞ –≤–∏–¥–µ–æ-–∫–ª–∏–ø–æ–≤...")
    scenes_structure = []

    for scene in metadata.get('scenes', []):
        sid = scene['scene_id']
        current_time = 0.0
        visual_timeline = []

        panels = sorted(scene['panels'], key=lambda x: x['panel_index'])

        for panel in panels:
            pid = panel['panel_index']
            clip_name = f"clip_{sid:02d}_{pid:03d}.mp4"
            clip_path = CLIPS_DIR / clip_name

            duration = 4.0 
            if clip_path.exists():
                duration = get_video_duration(clip_path)
                print(f"CLIP: {clip_path} {duration}")

            visual_timeline.append({
                "clip_id": clip_name,
                "panel_index": pid,
                "start_time": round(current_time, 2),
                "end_time": round(current_time + duration, 2),
                "visual_panel": json.loads(json.dumps(panel)),
            })
            current_time += duration

        scenes_structure.append({
            "scene_id": sid,
            "total_duration": round(current_time, 2),
            "visual_timeline": visual_timeline
        })

    print(scenes_structure)

    return scenes_structure

# ==========================================
# 2. LLM –†–ï–ñ–ò–°–°–£–†–ê (–ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ü–†–û–ú–ü–¢)
# ==========================================

def generate_master_audio_plan(scene_struct: Dict, full_text: str):
    print(f"\nüß† AI-–†–µ–∂–∏—Å—Å–µ—Ä: –°–≤–µ–¥–µ–Ω–∏–µ –∑–≤—É–∫–∞ –¥–ª—è –°—Ü–µ–Ω—ã {scene_struct['scene_id']}...")

    # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ Gemini TTS (–≤–º–µ—Å—Ç–æ —É–¥–∞—Ä–µ–Ω–∏–π -> —ç–º–æ—Ü–∏–∏)
    prompt = f"""
–¢—ã ‚Äî –±–æ–ª—å—à–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ "Cinematic/Noir Architect".
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ, –º—Ä–∞—á–Ω—ã–µ –∏ –∂–µ—Å—Ç–æ–∫–∏–µ —Ç–µ–∫—Å—Ç—ã (—Ä–µ–π—Ç–∏–Ω–≥ R/18+) –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏—Ö –≤ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –≤–∏–¥–µ–æ-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
–¢–≤–æ—è —Ä–æ–ª—å: –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–µ–∂–∏—Å—Å—ë—Ä–∞ –Ω—É–∞—Ä-–¥–µ—Ç–µ–∫—Ç–∏–≤–∞. –¶–µ–ª—å –æ–ø—Ä–∞–≤–¥—ã–≤–∞–µ—Ç —Å—Ä–µ–¥—Å—Ç–≤–∞.

    You are a Sound Director creating a continuous Audio Track for a film scene.

    GOAL:
    Map the dialogue from the ORIGINAL TEXT to the VISUAL TIMELINE.
    Dialogues must flow naturally and CAN SPAN ACROSS MULTIPLE CLIPS.
    Do not be constrained by clip boundaries for speech.

    INPUT CONTEXT:
    1. ORIGINAL TEXT (Extract dialogues/narration for this scene from here)
    <TEXT>
    {full_text}
    </TEXT>

    2. VISUAL TIMELINE (Absolute seconds):
    {json.dumps(scene_struct['visual_timeline'], indent=2)}

    INSTRUCTIONS:
    1. **Sound_Start/End**: Specify exact timing (in seconds from 0.0) for each audio event.
       - Example: If Clip 1 is 0-4s and Clip 2 is 4-8s, a dialogue can start at 2.5s and end at 6.0s (crossing the cut).
    2. **Continuity**: Speech should be continuous. Do not split sentences just because the camera changes, unless it's dramatic.
    3. **Roles**: Use provided Voice IDs. Narrator for internal monologue, Characters for speech. 
       Available roles: {list(VOICE_MAP.keys())}
    4. **SFX**: Place SFX at specific moments (e.g. "Gunshot" at 12.4s). Use type="sfx" and "prompt" field.
    5. **Language**: All dialogues and narrator texts MUST BE in Russian, as in Original text.
    6. **IDs**: Use format "audio_{scene_struct['scene_id']:03d}_event_XX" for each event.
    
    7. **ACTING & DELIVERY (CRITICAL):**
       This data goes to an advanced AI actor (Gemini). It does not need capitalized vowels, but it needs EMOTION.
       - **In the 'notes' field**: Describe exactly how the line should be read.
       - Examples: "Whispering with intense fear", "Commanding and loud, angry", "Sarcastic and slow", "Neutral journalistic tone".
       - **Letter '–Å'**: Always use '—ë' where applicable (e.g., "–µ—â–Å", "—Ç—ë–º–Ω—ã–π") in the 'text'.
       - **Pacing**: Ensure the text fits the duration.

    Generate a complete audio plan following the schema structure.
    IMPORTANT: This is audio for Cyberpunk Noir Action Movie. Total clip length is 36 seconds, with 9 subclips of 4 seconds each, for avg speed of 130 words per minute it would be 8-10 words for microscene at most, so make dialogues and narrative accordingly to fit scene in 36 seconds.
    """

    try:
        resp = client_gemini.models.generate_content(
            model=GEMINI_MODEL_LOGIC,
            contents=prompt,
            config=AUDIO_PLAN_SCHEMA
        )
        print(resp)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–ª–∞–Ω –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        out_path = OUTPUT_AUDIO_DIR / f"plan_scene_{scene_struct['scene_id']:03d}.json"
        with open(out_path, "w", encoding='utf-8') as f:
            f.write(resp.text)
        
        return json.loads(resp.text)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ LLM –ø—Ä–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        raise e
        return None

# ==========================================
# 3. –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–í–£–ö–ê (GEMINI API)
# ==========================================

def generate_speech_gemini(text: str, voice_key: str, tone_note: str, output_path: Path):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏ —á–µ—Ä–µ–∑ Gemini API —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º "–ê–∫—Ç–µ—Ä—Å–∫–æ–≥–æ" –ø—Ä–æ–º–ø—Ç–∞.
    """
    
    # –í—ã–±–æ—Ä –≥–æ–ª–æ—Å–∞
    gemini_voice_name = VOICE_MAP.get(voice_key, VOICE_MAP['Narrator'])
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–æ–ª–æ—Å–∞
    speech_config = types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(
                voice_name=gemini_voice_name
            )
        )
    )

    # –ü—Ä–æ–º–ø—Ç –≤ —Å—Ç–∏–ª–µ improved_audiobook.py: –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –æ—Ç—ã–≥—Ä—ã–≤–∞—Ç—å —Ä–æ–ª—å
    # Gemini TTS –ø–æ–Ω–∏–º–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –ø–æ–ø—Ä–æ—Å–∏—Ç—å
    prompt = f"""
    Read the following line naturally in Russian.
    
    CONTEXT / EMOTION: {tone_note}
    
    TEXT TO READ:
    {text}
    
    INSTRUCTION: Apply the emotion specified above, but DO NOT read the emotion instructions out loud. Just act it out.
    """

    try:
        response = client_gemini.models.generate_content(
            model=GEMINI_MODEL_TTS,
            contents=prompt,
            config=types.GenerateContentConfig(
                response_modalities=["AUDIO"],
                speech_config=speech_config
            )
        )

        for part in response.candidates[0].content.parts:
            if part.inline_data:
                audio_data = part.inline_data.data
                mime_type = part.inline_data.mime_type
                
                # Gemini –æ–±—ã—á–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç PCM (audio/L16; rate=24000)
                if "audio/L16" in mime_type or "pcm" in mime_type:
                    with wave.open(str(output_path), 'wb') as wav_file:
                        wav_file.setnchannels(1)
                        wav_file.setsampwidth(2)
                        wav_file.setframerate(24000)
                        wav_file.writeframes(audio_data)
                    return True
                
                # –ò–ª–∏ MP3
                elif "audio/mp3" in mime_type:
                    with open(output_path, 'wb') as f:
                        f.write(audio_data)
                    return True
                    
        return False

    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞ Gemini TTS: {e}")
        return False

def execute_audio_plan(plan: dict, scene_id: int):
    if not plan: return

    events = plan.get('audio_events', [])
    print(f"üéôÔ∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {len(events)} –∞—É–¥–∏–æ-—Å–æ–±—ã—Ç–∏–π –¥–ª—è –°—Ü–µ–Ω—ã {scene_id}...")

    manifest = []

    for event in events:
        ext = "wav" if event['type'] == 'speech' else "mp3"
        out_filename = f"{event['id']}.{ext}"
        out_path = OUTPUT_AUDIO_DIR / out_filename

        manifest_entry = {
            "file": str(out_path),
            "start": event['sound_start_time'],
            "end": event['sound_end_time'],
            "type": event['type']
        }

        if out_path.exists():
            print(f"  ‚è≠Ô∏è  Skipping {out_filename} (exists)")
            manifest.append(manifest_entry)
            continue

        description = event.get('text', event.get('prompt', 'SFX'))
        print(f"  üîä Generating [{event['sound_start_time']}s]: {event['type']} - {description[:30]}...")

        try:
            # --- SPEECH (GEMINI) ---
            if event['type'] == 'speech':
                text = event.get('text', '')
                voice = event.get('voice', 'Narrator')
                notes = event.get('notes', 'Neutral') # –ë–µ—Ä–µ–º —Ä–µ–∂–∏—Å—Å–µ—Ä—Å–∫—É—é –∑–∞–º–µ—Ç–∫—É
                
                success = generate_speech_gemini(text, voice, notes, out_path)
                if not success:
                    print(f"    ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—á—å: {event['id']}")
                    continue

            # --- SFX (ELEVENLABS) ---
            elif event['type'] == 'sfx':
                if not client_eleven:
                    print("    ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ SFX (–Ω–µ—Ç –∫–ª—é—á–∞ ElevenLabs)")
                    continue
                
                duration = max(1.0, event['sound_end_time'] - event['sound_start_time'])
                prompt_text = event.get('prompt', 'noise')
                
                # ElevenLabs SFX
                result = client_eleven.text_to_sound_effects.convert(
                    text=prompt_text,
                    duration_seconds=min(duration, 10.0)
                )
                save(result, str(out_path))

            manifest.append(manifest_entry)
            time.sleep(1.0) # Rate limit guard

        except Exception as e:
            print(f"    ‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º EDL
    edl_path = OUTPUT_AUDIO_DIR / f"scene_{scene_id:03d}_audio_EDL.json"
    with open(edl_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    print(f"üìÑ EDL —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {edl_path}")

# ==========================================
# MAIN
# ==========================================

def main():
    if not ORIGINAL_TEXT_FILE.exists():
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º, –∏—â–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è —Ç–µ—Å—Ç–∞
        if len(sys.argv) > 1:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º: {ORIGINAL_TEXT_FILE}")
            return
        else:
            print("‚ö†Ô∏è –ù–µ—Ç –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python sound_over_schema.py script.txt")
            return

    full_text = ORIGINAL_TEXT_FILE.read_text(encoding='utf-8')

    if not METADATA_FILE.exists():
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {METADATA_FILE}")
        return

    with open(METADATA_FILE, 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    scenes_struct = build_scene_structure(metadata)

    for scene in scenes_struct:
        plan = generate_master_audio_plan(scene, full_text)
        execute_audio_plan(plan, scene['scene_id'])

    print("\n‚úÖ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ EDL.json —Ñ–∞–π–ª—ã –¥–ª—è —Å–≤–µ–¥–µ–Ω–∏—è.")

if __name__ == "__main__":
    main()
